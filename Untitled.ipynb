{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444928f1-9e06-4448-9134-27fbbef88496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_gene(tokenizer):\n",
    "    #data argument 처리 해줘야 할것들\n",
    "    padding= False\n",
    "    doc_stride = 128\n",
    "    max_length =384\n",
    "    target_length = 128\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    def tokenized_samples(examples):\n",
    "        print(len(examples['question']))\n",
    "        tokenized_examples = tokenizer( examples['question'],examples['context'],\n",
    "            max_length=max_length,\n",
    "            truncation=\"only_second\",\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            stride=doc_stride )   \n",
    "        \n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "        # Let's label those examples!\n",
    "        tokenized_examples['labels']=[]\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[\"answers\"][sample_index]\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                tokenized_lables = tokenizer(answers['text'],\n",
    "                    max_length=target_length,\n",
    "                    padding=padding,\n",
    "                    truncation=True)   \n",
    "            # If no answers are given, set the cls_index as answer.)\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples['labels'].append([''])\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples['labels'].append([2])\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples['labels'].append(tokenized_lables['input_ids'][0])\n",
    "        return tokenized_examples\n",
    "    return tokenized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284add4-4f29-4666-9e20-62772385c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pre_process_gene(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0efbfa-ac26-4e71-a0cc-67d7048a427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = train_data.map( preprocess  , num_proc_workers= 4 ,,.....)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
